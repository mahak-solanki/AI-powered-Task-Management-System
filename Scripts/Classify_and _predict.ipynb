{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15aee99a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Data_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mData_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_text\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Data_analysis'"
     ]
    }
   ],
   "source": [
    "from Data_analysis import preprocess_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 4. Encode labels\n",
    "status_encoder = LabelEncoder()\n",
    "y_status = status_encoder.fit_transform(statuses)\n",
    "\n",
    "priority_encoder = LabelEncoder()\n",
    "y_priority = priority_encoder.fit_transform(priorities)\n",
    "\n",
    "# 5. Train-test split\n",
    "X_train, X_test, y_train_status, y_test_status = train_test_split(X, y_status, test_size=0.3, random_state=42)\n",
    "_, _, y_train_priority, y_test_priority = train_test_split(X, y_priority, test_size=0.3, random_state=42)\n",
    "\n",
    "# 6. Train models\n",
    "status_model = SVC()\n",
    "priority_model = RandomForestClassifier()\n",
    "\n",
    "status_model.fit(X_train, y_train_status)\n",
    "priority_model.fit(X_train, y_train_priority)\n",
    "\n",
    "# 7. Test new task\n",
    "task = input(\"Enter task description: \")\n",
    "clean_task = preprocess_text(task)\n",
    "vec_task = vectorizer.transform([clean_task])\n",
    "\n",
    "pred_status = status_encoder.inverse_transform(status_model.predict(vec_task))[0]\n",
    "pred_priority = priority_encoder.inverse_transform(priority_model.predict(vec_task))[0]\n",
    "\n",
    "print(f\"\\nüìù Task: {task}\")\n",
    "print(f\"üìå Predicted Status: {pred_status}\")\n",
    "print(f\"‚ö†Ô∏è Predicted Priority: {pred_priority}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97da578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Predict using Naive Bayes\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes Performance:\\n\")\n",
    "print(classification_report(y_test, nb_preds, target_names=le.classes_))\n",
    "\n",
    "# Predict using SVM\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "print(\"SVM Performance:\\n\")\n",
    "print(classification_report(y_test, svm_preds, target_names=le.classes_))\n",
    "\n",
    "# Simulate priority labels\n",
    "import random\n",
    "\n",
    "dataset['priority'] = [random.choice(['Low', 'Medium', 'High']) for _ in range(len(dataset))]\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "priority_encoder = LabelEncoder()\n",
    "dataset['priority_encoded'] = priority_encoder.fit_transform(dataset['priority'])\n",
    "print(priority_encoder.classes_)  # ['High', 'Low', 'Medium']\n",
    "\n",
    "# You can reuse the TF-IDF features from earlier as X\n",
    "X_priority = X\n",
    "y_priority = dataset['priority_encoded']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_priority, y_priority, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "print(\"Random Forest Priority Prediction:\\n\")\n",
    "print(classification_report(y_test, rf_preds, target_names=priority_encoder.classes_))\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "print(\"XGBoost Priority Prediction:\\n\")\n",
    "print(classification_report(y_test, xgb_preds, target_names=priority_encoder.classes_))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Count tasks per user\n",
    "user_workload = dataset['assignee'].value_counts().to_dict()\n",
    "print(\"Current Workload:\\n\", user_workload)\n",
    "\n",
    "# Assign new task to least-busy user\n",
    "new_task = \"optimize task assignment logic\"\n",
    "least_busy = min(user_workload, key=user_workload.get)\n",
    "print(f\"Assign '{new_task}' to: {least_busy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
